{
 "metadata": {
  "name": "",
  "signature": "sha256:093b2ad6fab9388eb36e054fc434d3f289bc1121bbabf9bad2495179c7101742"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Theano Basics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "import theano\n",
      "import numpy as np\n",
      "\n",
      "from theano import tensor as T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convention:\n",
      "#  uppercase: symbolic theano element or function\n",
      "#  lowercase: numpy array\n",
      "W = T.vector('w')\n",
      "X = T.matrix('X')\n",
      "Y = X.dot(W)\n",
      "F = theano.function([W,X], Y)\n",
      "\n",
      "w = np.ones(4)\n",
      "x = np.ones((10,4))\n",
      "y = F(w,x)\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The most underused tool in machine learning\n",
      "# AUTODIFF\n",
      "grad_w = T.grad(Y.sum(), W)\n",
      "F_grad = theano.function([W,X], grad_w)\n",
      "g = F_grad(w,x)\n",
      "# this should be equal to the sum of the columns of X (do you know how to matrix calculus?)\n",
      "print(g)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# An easier example\n",
      "B = T.scalar('E')\n",
      "R = T.sqr(B)\n",
      "A = T.grad(R, B)\n",
      "Z = theano.function([B], A)\n",
      "i = 2\n",
      "l = Z(i)\n",
      "print(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If that didn't blow your mind, well, it should have.\n",
      "B = theano.shared(np.ones(2))\n",
      "R = T.sqr(B).sum()\n",
      "A = T.grad(R, B)\n",
      "Z = theano.function([], R, updates={B: B - .1*A})\n",
      "for i in range(10):\n",
      "    print('cost function = {}'.format(Z()))\n",
      "    print('parameters    = {}'.format(B.get_value()))\n",
      "# Try to change range to 100 to see what happens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Nets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Now that we now how to sum, we have enough to Deep Learn\n",
      " ... I should say something in the board about the Model-View-Controller way we usually\n",
      "     deep learn with Theano.\n",
      "     Model      : Neural net parameters and dataset generator\n",
      "     View       : Logging, graph updates, saving cross-validated best parameters\n",
      "     Controller : Update algorithm that follows gradient directions to optimize paramters\n",
      "\n",
      " Download this dataset : http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      " \n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import cPickle\n",
      "from pylab import imshow\n",
      "\n",
      "train_set, valid_set, test_set = cPickle.load(file('mnist.pkl', 'r'))\n",
      "print(len(train_set))\n",
      "train_x, train_y = train_set\n",
      "test_x , test_y  = test_set\n",
      "print(train_x.shape)\n",
      "print(train_y.shape)\n",
      "_ = imshow(train_x[0].reshape((28,28)), cmap='gray')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def batch_iterator(x, y, batch_size):\n",
      "    num_batches = x.shape[0] // batch_size\n",
      "    for i in xrange(0,num_batches):\n",
      "        # TODO: use random integers instead of consecutive\n",
      "        #   values to avoid biased gradients\n",
      "        first = i * batch_size\n",
      "        last  = (i+1) * batch_size\n",
      "        x_batch = x[first:last]\n",
      "        y_pre   = y[first:last]\n",
      "        y_batch = np.zeros((batch_size, 10))\n",
      "        for row, col in enumerate(y_pre):\n",
      "            y_batch[row, col] = 1\n",
      "        yield (x_batch, y_batch)\n",
      "\n",
      "for x,y in batch_iterator(train_x, train_y, 10000):\n",
      "    print('{}, {}'.format(x.shape, y.shape))\n",
      "print(y[0])\n",
      "_ = imshow(x[0].reshape((28,28)), cmap='gray')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define layers\n",
      "def rectifier(input_dim, output_dim, X):\n",
      "    W = theano.shared(np.random.normal(0, .001, size=(input_dim, output_dim)))\n",
      "    b = theano.shared(np.zeros((output_dim,)))\n",
      "    Z = T.dot(X,W) + b.dimshuffle('x',0)\n",
      "    O = T.switch(Z>0, Z, 0)\n",
      "    return W,b,O\n",
      "\n",
      "def softmax(input_dim, output_dim, X, Y):\n",
      "    W = theano.shared(np.random.normal(0, .001, size=(input_dim, output_dim)))\n",
      "    b = theano.shared(np.zeros((output_dim,)))\n",
      "    Z = T.dot(X,W) + b.dimshuffle('x',0)\n",
      "    O = T.nnet.softmax(Z)\n",
      "    cost = T.nnet.binary_crossentropy(O, Y).sum()\n",
      "    return W,b,O,cost\n",
      "\n",
      "X = T.matrix('X')\n",
      "Y = T.matrix('Y')\n",
      "W0, b0, O0 = rectifier(784, 100, X)\n",
      "W1, b1, O1 = rectifier(100, 100, O0)\n",
      "W2, b2, O2, cost = softmax(100, 10,  O1, Y)\n",
      "\n",
      "# Always write tests\n",
      "F = theano.function([X,Y], [cost, O2])\n",
      "x = np.zeros((100,784))\n",
      "y = np.ones((100,10))\n",
      "c, z = F(x,y)\n",
      "assert c>0\n",
      "assert z.shape == (100,10)\n",
      "print(z[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import OrderedDict\n",
      "params = [W0, b0, W1, b1, W2, b2]\n",
      "updates = dict()\n",
      "for p in params:\n",
      "    updates[p] = p - .01 * T.grad(cost, p)\n",
      "updates = OrderedDict(updates)\n",
      "trainer = theano.function([X,Y], cost, updates=updates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_epochs = 100\n",
      "for i in range(num_epochs):\n",
      "    print('-'*10)\n",
      "    print('Epoch: {}'.format(i))\n",
      "    for iter,b in enumerate(batch_iterator(train_x, train_y, 128)):\n",
      "        x = b[0]\n",
      "        y = b[1]\n",
      "        last_cost = trainer(x,y)\n",
      "    print('cost: {}'.format(trainer(x,y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w0 = W0.get_value()\n",
      "_ = imshow(w0[:,0].reshape((28,28)), cmap='gray')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ERR = T.neq(O2.argmax(axis=-1), Y.argmax(axis=-1))\n",
      "Ferr = theano.function([X,Y], ERR)\n",
      "def testnet(x, y):\n",
      "    testerr = 0.\n",
      "    for b1,b2 in batch_iterator(x, y, 500):\n",
      "        testerr += Ferr(b1,b2)\n",
      "    return testerr.sum()\n",
      "\n",
      "print('test error: {}, test acc: {}'.format(testnet(test_x, test_y),\n",
      "       1 - testnet(test_x, test_y) / 10000.))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Convolutional Nets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      " We can do much better than this with more hidden neurons and dropout.\n",
      " Watch Alec Radford's presentation to see how to do that \n",
      " with Python/Theano: https://www.youtube.com/watch?v=S75EdAcXHKk\n",
      " For now, lets move on to convnets.\n",
      " \n",
      "\"\"\"\n",
      "from theano.tensor.nnet.conv import conv2d\n",
      "from theano.tensor.signal.downsample import max_pool_2d\n",
      "def conv_rectifier(input_channels, output_channels, filter_dim, X):\n",
      "    W = theano.shared(np.random.normal(0, .001, size=(output_channels,\n",
      "                                                      input_channels,\n",
      "                                                      filter_dim,\n",
      "                                                      filter_dim)))\n",
      "    b  = theano.shared(np.zeros((output_channels,)))\n",
      "    Z  = conv2d(X,W) + b.dimshuffle('x',0,'x','x')\n",
      "    DS = max_pool_2d(Z, ds=[2,2])\n",
      "    O  = T.switch(DS>0, DS, 0)\n",
      "    return W,b,O\n",
      "\n",
      "# test\n",
      "X = T.tensor4('X')\n",
      "W, b, O = conv_rectifier(1, 9, 5, X)\n",
      "F = theano.function([X], O)\n",
      "\n",
      "x = np.ones((5, 1, 28, 28))\n",
      "print(x.shape)\n",
      "o = F(x)\n",
      "o.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = T.matrix('Y')\n",
      "W0, b0, O0 = conv_rectifier(1, 20, 5, X)\n",
      "W1, b1, O1 = conv_rectifier(20, 50, 5, O0)\n",
      "\n",
      "# test\n",
      "F = theano.function([X], O1)\n",
      "o = F(x)\n",
      "print(o.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W2, b2, O2 = rectifier(50*4*4, 500, O1.flatten(2))\n",
      "W3, b3, O3, cost = softmax(500, 10,  O2, Y)\n",
      "# Teeeeeest\n",
      "x = np.ones((128,1,28,28))\n",
      "y = np.ones((128,10))\n",
      "F = theano.function([X, Y], [O3, cost])\n",
      "z, c = F(x,y)\n",
      "assert c>0\n",
      "assert z.shape == (128,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We need to modify the batch_iterator slightly to serve formated images\n",
      "def batch_iterator(x, y, batch_size):\n",
      "    num_batches = x.shape[0] // batch_size\n",
      "    for i in xrange(0,num_batches):\n",
      "        # TODO: use random integers instead of consecutive\n",
      "        #   values to avoid biased gradients\n",
      "        first = i * batch_size\n",
      "        last  = (i+1) * batch_size\n",
      "        x_batch = x[first:last].reshape((batch_size,1,28,28))\n",
      "        y_pre   = y[first:last]\n",
      "        y_batch = np.zeros((batch_size, 10))\n",
      "        for row, col in enumerate(y_pre):\n",
      "            y_batch[row, col] = 1\n",
      "        yield (x_batch, y_batch)\n",
      "\n",
      "for x,y in batch_iterator(train_x, train_y, 10000):\n",
      "    print('{}, {}'.format(x.shape, y.shape))\n",
      "print(y[0])\n",
      "_ = imshow(x[0].reshape((28,28)), cmap='gray')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = [W0, b0, W1, b1, W2, b2, W3, b3]\n",
      "updates = dict()\n",
      "for p in params:\n",
      "    updates[p] = p - .01 * T.grad(cost, p)\n",
      "updates = OrderedDict(updates)\n",
      "trainer = theano.function([X,Y], cost, updates=updates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_epochs = 100\n",
      "for i in range(num_epochs):\n",
      "    print('-'*10)\n",
      "    print('Epoch: {}'.format(i))\n",
      "    for iter,b in enumerate(batch_iterator(train_x, train_y, 128)):\n",
      "        x = b[0]\n",
      "        y = b[1]\n",
      "        last_cost = trainer(x,y)\n",
      "    print('cost: {}'.format(trainer(x,y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w0 = W0.get_value()\n",
      "_ = imshow(w0[0,0,:,:].reshape((5,5)), cmap='gray')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ERR = T.neq(O3.argmax(axis=-1), Y.argmax(axis=-1))\n",
      "Ferr = theano.function([X,Y], ERR)\n",
      "def testnet(x, y):\n",
      "    testerr = 0.\n",
      "    for b1,b2 in batch_iterator(x, y, 500):\n",
      "        testerr += Ferr(b1,b2)\n",
      "    return testerr.sum()\n",
      "\n",
      "print('test error: {}, test acc: {}'.format(testnet(test_x, test_y),\n",
      "       1 - testnet(test_x, test_y) / 10000.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}